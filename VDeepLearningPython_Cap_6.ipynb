{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlayerB96/API-PRUEBA/blob/master/VDeepLearningPython_Cap_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp37LD6fI5Er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24aab16d-107f-4262-91b6-743dec031b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1. 1. 1. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "#Listing 6.1 - Codificación one-hot a nivel de palabra \n",
        "import numpy as np\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1\n",
        "  max_length = 10\n",
        "  results = np.zeros(shape=(len(samples),\n",
        "      max_length,\n",
        "      max(token_index.values()) + 1))\n",
        "  for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "      index = token_index.get(word)\n",
        "      results[i, j, index] = 1\n",
        "      print(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.2 - Codificación one-hot de nivel de carácter\n",
        "import string\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1\n",
        "    print(results)"
      ],
      "metadata": {
        "id": "z0Y94SMb4COj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67cabebd-8aff-4ca9-d3ee-c5cf63cf9be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.3 - Uso de Keras para la codificación one-hot a nivel de palabra\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "id": "dlXXBdPd4c-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7f396c-3a5e-4b17-ac60-d1328b17334e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.4 - Codificación one-hot a nivel de palabra con truco hash\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = abs(hash(word)) % dimensionality\n",
        "    results[i, j, index] = 1\n",
        "    print(results)"
      ],
      "metadata": {
        "id": "CF_8jz0J4wWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaab2f9-aa83-4ec1-c358-0e758d648e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.5 - Instanciación de una capa incrustada\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000, 64)\n"
      ],
      "metadata": {
        "id": "2zlXk_cEHMJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.6 - Cargar los datos de IMDB para usarlos con una capa de incrustación\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences\n",
        "import keras\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
        "num_words=max_features)\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = keras.utils.pad_sequences(x_test, maxlen=maxlen)\n",
        "print(x_train)\n",
        "print(x_test)"
      ],
      "metadata": {
        "id": "5N-NjnSKHvCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd501f7-2287-4a59-f1c8-b97ac6c8cc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  65   16   38 ...   19  178   32]\n",
            " [  23    4 1690 ...   16  145   95]\n",
            " [1352   13  191 ...    7  129  113]\n",
            " ...\n",
            " [  11 1818 7561 ...    4 3586    2]\n",
            " [  92  401  728 ...   12    9   23]\n",
            " [ 764   40    4 ...  204  131    9]]\n",
            "[[ 286  170    8 ...   14    6  717]\n",
            " [  10   10  472 ...  125    4 3077]\n",
            " [  34    2   45 ...    9   57  975]\n",
            " ...\n",
            " [ 226   20  272 ...   21  846 5518]\n",
            " [  55  117  212 ... 2302    7  470]\n",
            " [  19   14   20 ...   34 2005 2643]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.7 - Uso de una capa de incrustación y un clasificador en los datos de IMDB\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "history = model.fit(x_train, y_train,\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "id": "CaEBO8aQObtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9632a6-c8d8-4bc2-cbfc-b8d5505d7a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 3s 3ms/step - loss: 0.6565 - acc: 0.6484 - val_loss: 0.5937 - val_acc: 0.7046\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5230 - acc: 0.7569 - val_loss: 0.5155 - val_acc: 0.7326\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4553 - acc: 0.7903 - val_loss: 0.4991 - val_acc: 0.7440\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4220 - acc: 0.8080 - val_loss: 0.4905 - val_acc: 0.7556\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3993 - acc: 0.8189 - val_loss: 0.4905 - val_acc: 0.7578\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3804 - acc: 0.8305 - val_loss: 0.4918 - val_acc: 0.7586\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3636 - acc: 0.8406 - val_loss: 0.4956 - val_acc: 0.7592\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3471 - acc: 0.8515 - val_loss: 0.5023 - val_acc: 0.7536\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.3313 - acc: 0.8597 - val_loss: 0.5064 - val_acc: 0.7556\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3154 - acc: 0.8666 - val_loss: 0.5130 - val_acc: 0.7508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.8 - Procesando las etiquetas de los datos IMDB sin procesar\n",
        "import os\n",
        "imdb_dir = 'sample_data/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      \n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "        print(labels)\n",
        "      else:\n",
        "        labels.append(1)\n",
        "        print(labels)"
      ],
      "metadata": {
        "id": "H7FPBg-oOc_D",
        "outputId": "88281a4c-5159-4464-c04a-a2f6831a2c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[0, 0]\n",
            "[0, 0, 0]\n",
            "[0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.9 - Tokenización del texto de los datos IMDB sin procesar\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "maxlen = 100\n",
        "training_samples = 200\n",
        "validation_samples = 10000\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(sequences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Encontrados %s unique tokens.' % len(word_index))\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print('Forma de data tensor:', data.shape)\n",
        "print('Forma de label tensor:', labels.shape)\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "id": "RFxHO86SP9ZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "febfe282-922d-405e-d954-15eb3357ecf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12, 10, 32, 5, 42, 77, 11, 56, 4, 439, 64, 260, 21, 56, 440, 441, 108, 2, 442, 90, 11, 261, 36, 57, 262, 132, 37, 91, 30, 1, 263, 443, 14, 92, 444, 78, 7, 445, 1, 13, 10, 446, 6, 177, 1, 79, 264, 9, 12, 10, 265, 447, 448, 449, 450, 9, 265, 266, 7, 451, 2, 178, 10, 4, 19, 179, 8, 26, 180, 133, 20, 452, 181, 11, 56, 453, 93, 25, 15, 94, 6, 43, 21, 58, 454, 455, 456, 32, 5, 42, 12, 13, 10, 134, 6, 78, 24, 14, 26, 135, 2, 58, 95, 21, 12, 10, 109, 457, 21, 24, 14, 182, 267, 110, 6, 43, 80, 183, 58], [81, 22, 268, 38, 12, 13, 11, 8, 269, 7, 22, 37, 458, 180, 133, 36, 57, 184, 2, 136, 108, 459, 26, 460, 461, 462, 79, 7, 270, 463, 6, 23, 11, 111, 81, 270, 44, 65, 6, 1, 137, 2, 1, 13, 31, 464, 17, 2, 17, 8, 96, 465, 466, 97, 1, 467, 138, 139, 5, 1, 77, 21, 11, 140, 27, 271, 468, 65, 7, 469, 26, 7, 272, 23, 19, 51, 1, 260, 21, 8, 185, 186, 112, 268, 38, 12, 13, 45, 470, 19, 109, 19, 90, 1, 273, 113, 114, 37, 471, 1, 136, 108, 22, 472, 2, 1, 262, 22, 473, 274, 17, 34, 187, 474, 475, 8, 141, 12, 13, 4, 476, 5, 142, 477], [8, 187, 12, 13, 8, 478, 7, 18, 139, 275, 35, 4, 188, 276, 64, 479, 9, 189, 6, 98, 29, 115, 277, 13, 143, 278, 8, 82, 31, 480, 34, 187, 279, 2, 22, 280, 7, 144, 30, 8, 116, 190, 38, 34, 481, 28, 4, 191, 66, 192, 8, 31, 482, 190, 36, 4, 483, 5, 484, 2, 58, 485, 145, 8, 116, 39, 3, 3, 8, 486, 6, 190, 487, 193, 488, 37, 281, 31, 282, 489, 490, 138, 194, 19, 90, 99, 1, 66, 7, 44, 491, 195, 30, 492, 64, 83, 493, 146, 196, 2, 494, 9, 1, 283, 495, 17, 9, 284, 139, 496, 15, 285, 18, 64, 497, 30, 40, 286, 498, 4, 197, 198, 4, 499, 266, 3, 3, 287, 42, 261, 37, 11, 100, 52, 288, 147, 17, 2, 289, 500, 501, 72, 290, 1, 291, 5, 4, 502, 67, 100, 198, 18, 292, 6, 148, 503, 7, 504, 73, 199, 1, 139, 275, 3, 3, 1, 293, 41, 117, 23, 505, 1, 506, 58, 56, 200, 17, 294, 2, 14, 16, 6, 141, 507, 295, 508, 25, 140, 59, 509], [6, 23, 510, 511, 8, 149, 150, 58, 118, 296, 8, 3, 3, 512, 7, 4, 138, 142, 513, 93, 5, 132, 514, 515, 3, 3, 53, 516, 68, 297, 12, 20, 22, 4, 517, 6, 43, 24, 3, 3, 14, 201, 150, 57, 74, 9, 189, 6, 298, 94, 35, 42, 14, 101, 3, 3, 44, 59, 4, 518, 46, 5, 58, 3, 3, 1, 53, 44, 119, 519, 7, 520, 29, 299, 289, 521, 3, 3, 2, 56, 4, 522, 2, 523, 120, 524, 525, 202, 3, 3, 21, 84, 56, 4, 526, 527, 300, 151, 203, 528, 98, 7, 3, 3, 45, 30, 9, 32, 47, 529, 197, 84, 530, 204, 4, 3, 3, 531, 205, 532, 2, 533, 102, 29, 534, 535, 2, 3, 3, 300, 203, 7, 3, 3, 58, 119, 536, 301, 26, 7, 22, 206, 17, 64, 7, 22, 3, 3, 206, 17, 537, 207, 538, 1, 208, 82, 3, 3, 302, 5, 184, 47, 539, 302, 2, 540, 3, 3, 96, 541, 542, 9, 29, 196, 21, 303, 14, 152, 304, 3, 3, 1, 90, 14, 152, 177, 75, 153, 305, 2, 14, 306, 3, 3, 30, 1, 307, 543, 154, 103, 544, 545, 51, 103, 121, 3, 3, 104, 2, 183, 12, 13, 546, 74, 26, 12, 25, 15, 4, 547, 2, 3, 3, 4, 548], [549, 67, 40, 308, 13, 6, 298, 137, 550, 209, 46, 551, 68, 552, 35, 122, 273, 113, 60, 5, 105, 16, 553, 123, 6, 309, 12, 13, 155, 210, 83, 1, 554, 555, 211, 58, 556, 557, 1, 558, 212, 151, 69, 85, 72, 156, 33, 1, 559, 15, 60, 8, 52, 10, 11, 1, 560, 561, 4, 562, 563, 28, 69, 9, 1, 564, 565, 310, 197, 60, 9, 60, 8, 70, 44, 26, 1, 13, 93, 79, 1, 191, 566, 145, 15, 308, 1, 311, 10, 4, 567, 5, 568, 312, 2, 569, 11, 570, 6, 571, 1, 53, 6, 4, 572, 313, 105, 8, 152, 157, 124], [12, 20, 56, 1, 573, 5, 4, 44, 19, 53, 1, 108, 5, 1, 574, 2, 314, 5, 1, 213, 315, 214, 9, 1, 575, 576, 10, 1, 577, 5, 578, 2, 579, 12, 20, 580, 86, 4, 19, 53, 36, 155, 581, 316, 317, 4, 582, 5, 583, 2, 584, 318, 585, 2, 61, 586, 319, 587, 11, 16, 28, 65, 588, 28, 215, 589, 1, 320, 196, 15, 39, 45, 24, 1, 283, 590, 10, 591, 2, 1, 592, 15, 216, 321, 21, 1, 593, 10, 594, 2, 595, 217, 10, 19, 214, 103, 596, 68, 43, 18, 7, 322, 323, 10, 324, 21, 27, 274, 1, 325, 326, 158, 15, 32, 597, 1, 122, 326, 10, 32, 5, 1, 213, 315, 598, 9, 1, 320, 327, 2, 218, 17, 136, 18, 38, 61, 145, 9, 1, 328, 20, 7, 87, 16, 48, 40, 599, 20, 2, 217, 600, 32], [134, 8, 106, 57, 41, 8, 214, 106, 41, 57, 219, 6, 54, 94, 5, 601, 71, 329, 5, 1, 215, 125, 114, 57, 219, 6, 54, 94, 5, 58, 22, 27, 3, 3, 7, 22, 602, 3, 3, 603, 33, 604, 3, 3, 34, 605, 2, 8, 606, 97, 1, 220, 20, 2, 8, 16, 4, 607, 5, 330, 221, 9, 1, 12, 608, 331, 2, 9, 1, 609, 5, 610, 611, 3, 3, 137, 332, 5, 126, 4, 299, 9, 12, 222, 105, 10, 223, 224, 78, 612, 613, 33, 10, 614, 120, 2, 225, 102, 95, 333, 48, 305, 12, 18, 83, 4, 615, 85, 27, 147, 291, 6, 59, 616, 617, 618, 14, 47, 65, 223, 61, 71, 155, 211, 32, 334, 4, 619, 2, 1, 335, 15, 31, 144, 620, 9, 4, 336, 33, 621, 622, 6, 337, 28, 4, 623, 21, 11, 72, 338, 86, 112, 3, 3, 624, 159, 625, 99, 83, 226, 75, 155, 626, 627, 226, 628, 4, 629, 630, 263, 631, 23, 339, 340, 632, 633, 3, 3, 124, 12, 336, 634, 1, 135, 341, 33, 1, 335, 16, 48, 635, 1, 220, 66, 2, 1, 61, 5, 42, 148, 6, 1, 313, 11, 49, 160, 161, 4, 135, 342, 33, 10, 225, 102, 95, 6, 636, 219, 343, 6, 54, 337, 4, 121, 2, 80, 637, 99, 83, 1, 638, 71, 223, 31, 104, 6, 639, 3, 3, 640, 3, 3, 227, 162, 10, 12, 641, 73, 24, 228, 27, 51, 224, 14, 272, 27, 642, 11, 35, 1, 318, 643, 644, 55, 38, 645, 5, 1, 13, 646, 2, 50, 344, 345, 126, 4, 647, 18, 4, 648, 649, 650, 2, 11, 9, 1, 651, 652, 653, 654, 655, 312, 28, 24, 346, 1, 61, 656, 657, 1, 181, 33, 59, 50, 95, 229, 102, 38, 658, 145, 204, 1, 347, 25, 659, 1, 110, 660, 105, 230, 127, 16, 48, 40, 661, 30, 12, 13, 22, 231, 3, 3, 348, 51, 224, 8, 662, 17, 1, 663, 664, 2, 665, 34, 334, 9, 666, 3, 3, 134, 667, 162, 7, 154, 44, 668, 348, 4, 41, 13, 669, 27, 670, 70, 104, 671, 18, 34, 349, 9, 1, 294, 8, 98, 143, 672, 5, 143, 2, 673, 1, 674, 5, 12, 13, 8, 52, 11, 160, 161, 67, 4, 675, 350, 342, 33, 351, 1, 676, 2, 56, 677, 48, 678, 55, 352, 353, 679, 36, 680, 10, 96, 4, 681, 26, 682, 683, 3, 3, 9, 128, 684, 84, 119, 229, 102, 95, 3, 3, 685, 4, 276, 686, 5, 1, 687, 688, 10, 11, 160, 161, 92, 16, 4, 121, 689, 84, 354, 50, 690, 691, 327, 120, 1, 692, 5, 352, 345, 5, 1, 693, 55, 1, 350, 3, 3, 27, 6, 694, 24, 333, 163, 695, 292, 296, 54, 4, 110, 121, 3, 3, 164, 24, 160, 161, 696, 4, 121, 84, 232, 104, 120, 225, 102, 95, 697, 59, 29, 19, 698, 2, 699, 2, 700, 701, 3, 3, 9, 702, 1, 79, 355, 25, 140, 59, 703, 159, 10, 11, 9, 1, 233, 704, 356, 705, 706, 357, 79, 6, 290, 11, 1, 135, 341, 10, 707, 25, 234, 708, 86, 1, 709, 5, 1, 710, 1, 111, 107, 77, 98, 112, 212, 358, 143, 711, 157, 1, 359, 712, 205, 713, 18, 714, 9, 356, 715, 1, 716, 14, 101, 717, 718, 719, 9, 106, 2, 60, 11, 720, 3, 3, 24, 14, 231, 7, 12, 721, 97, 34, 330, 8, 722, 14, 2, 85, 360, 112, 281, 23, 62, 360, 24, 14, 116, 1, 182, 6, 43, 1, 20, 17, 723, 5, 123, 37, 724, 70], [57, 68, 57, 184, 57, 137, 12, 82, 1, 361, 5, 4, 725, 13, 8, 16, 69, 62, 726, 207, 64, 5, 4, 362, 727, 235, 12, 13, 7, 728, 165, 66, 70, 363, 146, 66, 71, 364, 17, 12, 365, 49, 366, 11, 25, 729, 12, 730, 5, 731, 6, 45, 23, 732, 17, 64, 7, 166, 23, 4, 733], [8, 734, 6, 1, 13, 28, 4, 735, 736, 9, 737, 37, 234, 16, 40, 367, 75, 8, 738, 147, 6, 78, 1, 53, 10, 47, 739, 1, 13, 10, 47, 236, 8, 368, 7, 87, 16, 229, 6, 138, 195, 740, 741, 129, 369, 6, 1, 53, 132, 1, 277, 5, 13, 14, 92, 78, 9, 4, 362, 205, 105, 14, 237, 6, 59, 742, 62, 322, 323, 22, 47, 19, 743, 217, 10, 134, 1, 13, 10, 744, 38, 1, 167, 213, 351, 370, 25, 15, 745, 50, 746, 331, 2, 1, 369, 25, 747, 28, 81, 748, 65, 113, 9, 1, 13, 37, 24, 14, 237, 6, 43, 7, 80, 109, 182, 6, 363, 146, 364, 2, 66, 147, 6, 1, 749, 117, 23, 62, 371, 6, 43, 7, 35, 207, 30, 7, 154, 17, 750], [85, 156, 11, 1, 751, 17, 1, 372, 752, 753, 185, 16, 82, 4, 4, 19, 306, 71, 61, 30, 754, 373, 755, 756, 757, 12, 32, 17, 1, 374, 66, 758, 47, 759, 1, 760, 375, 1, 761, 762, 162, 1, 20, 763, 10, 764, 765, 31, 26, 1, 311, 5, 1, 376, 238, 71, 35, 377, 1, 376, 238, 22, 284, 766, 374, 109, 165, 239, 767, 81, 153, 65, 378, 768, 9, 372, 378, 235, 769, 770, 2, 771, 379, 87, 772, 6, 773, 86, 50, 774, 3, 3, 1, 20, 775, 36, 1, 61, 5, 42, 776, 9, 4, 777, 2, 30, 380, 381, 778, 4, 779, 5, 50, 780, 55, 27, 781, 42, 18, 782, 103, 239, 25, 783, 1, 239, 2, 118, 1, 382, 5, 1, 165, 784, 26, 785, 25, 786, 3, 3, 375, 1, 111, 379, 787, 788, 789, 790, 55, 791, 792, 2, 793, 36, 103, 65, 794, 795, 25, 164, 16, 29, 796, 383, 797, 36, 380, 381, 3, 3, 124, 167, 798, 33, 26, 384, 799, 70, 800, 36, 385, 801, 2, 49, 127, 4, 19, 386, 51, 1, 387, 802, 5, 803, 804, 33, 805, 60, 125, 806, 807, 384, 18, 808, 1, 809, 10, 4, 810, 2, 8, 388, 11, 811, 812, 813, 11, 166, 16, 814, 48, 357, 17, 40, 4, 387, 3, 3, 21, 8, 232, 16, 129, 21, 386, 389, 301, 35, 7], [18, 211, 815, 48, 150, 390, 41, 20, 11, 338, 46, 51, 1, 816, 391, 817, 26, 818, 195, 6, 819, 115, 820, 11, 15, 233, 821, 100, 822, 97, 60, 5, 57, 74, 9, 823, 5, 824, 168, 392, 26, 91, 130, 3, 3, 100, 825, 6, 826, 4, 393, 5, 12, 20, 35, 827, 828, 1, 169, 149, 7, 2, 8, 368, 7, 829, 86, 394, 1, 830, 18, 1, 122, 20, 3, 3, 67, 7, 230, 27, 16, 1, 133, 71, 264, 343, 5, 831, 74, 7, 832, 4, 833, 834, 9, 1, 170, 395, 2, 835, 836, 1, 68, 2, 240, 15, 396, 397, 21, 49, 1, 837, 11, 838, 12, 20, 199, 1, 309, 1, 53, 2, 158, 839, 35, 31, 1, 159, 840, 6, 841, 29, 398, 88, 3, 3, 1, 280, 2, 842, 843, 15, 164, 396, 397, 2, 67, 241, 41, 74, 70, 844, 6, 282, 845, 6, 50, 846, 1, 847, 18, 91, 130, 79, 848, 6, 49, 849, 3, 3, 8, 52, 1, 20, 56, 850, 35, 4, 851, 5, 852, 21, 316, 16, 48, 853, 123, 37, 8, 152, 198, 6, 304, 30, 854, 171, 23, 855, 46, 17, 399, 856, 123, 45, 24, 228, 857, 5, 1, 858, 859, 5, 41, 74, 141, 12, 32, 4, 860, 14, 242, 861, 7], [288, 19, 38, 7, 29, 862, 2, 400, 170, 168, 113, 11, 101, 59, 339, 1, 863, 5, 45, 1, 107, 864, 41, 243, 119, 865, 17, 1, 866, 867, 868, 18, 132, 869, 172, 45, 24, 244, 69, 297, 126, 127, 4, 245, 246, 9, 12, 20, 11, 101, 54, 146, 870, 871, 3, 3, 75, 87, 16, 48, 285, 38, 7, 1, 68, 22, 35, 210, 324, 2, 872, 7, 82, 4, 873, 17, 64, 361, 9, 29, 401, 247, 7, 303, 874, 1, 395, 7, 22, 875, 18, 2, 1, 136, 108, 10, 248, 5, 876, 877, 878, 5, 879, 880, 247, 1, 113, 15, 400, 170, 8, 249, 25, 230, 16, 402, 6, 1, 73, 4, 245, 403, 241, 210, 36, 29, 5, 42, 1, 404, 881, 882, 2, 883, 83, 75, 87, 16, 48, 62, 884, 172, 1, 405, 10, 885, 6, 886, 46, 4, 250, 5, 246, 18, 406, 27, 28, 4, 407, 55, 1, 208, 21, 93, 25, 31, 887, 7, 3, 3, 127, 4, 19, 168, 115, 41, 20, 11, 10, 408, 888, 889, 890, 1, 891, 73, 199, 1, 892], [24, 14, 251, 163, 69, 12, 13, 5, 173, 174, 2, 893, 80, 60, 8, 92, 186, 10, 75, 15, 14, 409, 18, 12, 10, 894, 193, 895, 2, 896, 13, 55, 1, 897, 89, 11, 100, 60, 52, 2, 106, 37, 65, 49, 94, 319, 898, 2, 899, 900, 901, 410, 902, 5, 411, 28, 412, 173, 174, 2, 252, 15, 1, 325, 410, 200, 124, 117, 129, 293, 243, 237, 7, 129, 128, 111, 5, 411, 27, 124, 49, 4, 39, 13, 9, 390, 903, 5, 1, 904, 37, 24, 14, 251, 163, 69, 7, 80, 14, 31, 16, 6, 144, 8, 388, 159, 144, 403, 37, 75, 15, 14, 409, 18, 8, 905, 11, 14, 242, 23, 253, 906, 907, 908], [12, 13, 10, 1, 122, 13, 151, 7, 56, 4, 250, 5, 909, 910, 49, 31, 39, 911, 166, 43, 7, 2, 1, 286, 15, 39, 1, 912, 10, 131, 913, 254, 1, 122, 914, 151, 1, 90, 15, 39, 173, 174, 63, 10, 915, 4, 39, 179, 178, 916, 917, 2, 254, 4, 39, 175, 2, 252, 63, 918, 919, 2, 254, 4, 39, 175, 6, 106, 30, 9, 131, 106, 7], [8, 412, 116, 920, 2, 921, 922, 7, 93, 85, 4, 188, 73, 923, 924, 925, 21, 8, 96, 332, 5, 26, 1, 63, 89, 2, 8, 926, 1, 77, 25, 54, 45, 247, 212, 72, 44, 48, 50, 413, 169, 30, 9, 131, 22, 4, 927, 173, 174, 2, 252, 13, 414, 36, 1, 415, 928, 929, 416, 930, 931, 28, 4, 373, 932, 255, 933, 2, 45, 1, 934, 935, 83, 1, 417, 8, 418, 936, 30, 9, 131, 2, 1, 128, 63, 419, 77, 72, 937, 6, 23, 420, 153, 27, 107, 5, 1, 66, 25, 79, 938, 17, 64, 2, 25, 72, 939, 6, 23, 1, 307, 940, 941, 71, 45, 40, 942, 6, 157, 34, 209, 10, 202, 33, 43, 12, 13, 2, 943, 7, 6, 23, 420, 128, 235, 193, 63, 419, 13, 101, 23, 253, 3, 3, 11, 944, 125, 33, 15, 389, 5, 1, 63, 89, 101, 44, 201, 406, 18, 125, 5, 167, 945, 946, 42, 278, 1, 191, 947, 5, 248, 329, 49, 44, 39, 6, 78, 42, 948, 204, 62, 949, 950, 12, 13, 951, 421, 952, 2, 385, 953, 31, 26, 241, 5, 50, 128, 77, 157, 142, 954, 955, 51, 394, 956, 2, 1, 957, 6, 1, 958, 51, 959, 6, 960, 28, 73, 28, 961, 19, 962, 94, 11, 92, 23, 418, 55, 1, 328, 344, 3, 3, 28, 236, 28, 8, 127, 116, 26, 85, 17, 34, 963, 2, 28, 236, 28, 8, 92, 54, 7, 964, 6, 1, 13, 422, 123, 99, 4, 965, 6, 407, 125, 33, 966, 1, 63, 89, 9, 189, 6, 23, 4, 243, 5, 1, 63, 89, 14, 70, 16, 6, 23, 29, 967, 968, 423, 969, 51, 970, 9, 971, 118, 27, 44, 1, 413, 169, 24, 7, 114, 1, 972, 4, 255, 973, 5, 974, 2, 975, 117, 23, 383, 97, 976, 71, 29, 977, 26, 11, 27, 978, 979, 30, 9, 131, 67, 7, 424, 279, 980, 255, 2, 981, 982, 2, 61, 256, 51, 4, 423, 9, 983, 984, 44, 38, 11, 49, 62, 38, 985, 257, 256, 33, 16, 986, 6, 422, 7, 99, 42, 401, 24, 11, 258, 227, 1, 13, 9, 29, 987, 415, 222, 36, 988, 295, 17, 416, 37, 23, 7, 118, 989, 990, 14, 99, 40, 367, 2, 991, 7, 55, 992, 7, 993, 35, 377, 153, 994, 4, 19, 995, 45, 24, 1, 258, 996, 4, 168, 997, 3, 3, 998, 70, 425, 1, 20, 426, 244, 69, 7, 2, 80, 70, 425, 7, 426, 244, 999, 6, 177, 75, 1, 63, 89, 98, 25, 1000, 257, 256, 6, 23, 1001, 1002, 2, 1003, 257, 314, 12, 13, 424, 11, 8, 249, 31, 26, 50, 1004, 1005, 201, 1006, 98, 1, 1007, 24, 14, 26, 1, 63, 89, 14, 242, 23, 253], [226, 60, 1, 365, 11, 391, 2, 1, 1008, 16, 1009, 46, 100, 171, 59, 4, 13, 11, 427, 29, 428, 172, 81, 15, 29, 1010, 172, 21, 85, 27, 156, 49, 1011, 429, 6, 54, 40, 1012, 1013, 13, 126, 27, 65, 110, 200, 49, 31, 358, 73, 3, 3, 54, 156, 2, 1014, 1015, 28, 1, 1016, 88, 148, 430, 2, 431, 12, 10, 27, 4, 13, 6, 43, 67, 228, 1017, 1018, 3, 3, 126, 32, 47, 73, 1019, 1020, 1021, 9, 12, 13, 2, 1022, 32, 47, 1023, 1024, 1025, 1026, 353, 31, 119, 54, 40, 371, 176, 1, 259, 1027, 18, 32, 355, 218, 432, 1028, 8, 52, 8, 52, 218, 31, 402, 97, 4, 1029, 1030, 21, 1, 405, 72, 154, 6, 52, 76, 28, 178, 1031, 10, 2, 1, 1032, 346, 76, 2, 76, 1033, 10, 433, 1034, 9, 40, 1035, 111, 8, 249, 11, 1, 192, 82, 1036, 18, 167, 6, 16, 29, 1037, 18, 76, 21, 8, 72, 140, 3, 3, 171, 4, 434, 86, 18, 1, 404, 105, 10, 221, 1038, 2, 317, 7, 87, 408, 16, 48, 1039, 36, 4, 1040, 1041, 35, 1, 382, 5, 1, 1042, 417, 21, 1, 192, 1043, 1044, 3, 3, 12, 10, 27, 4, 1045, 55, 129, 258, 21, 7, 10, 4, 19, 359, 1046, 428, 13, 267, 118, 433, 1047, 429], [12, 13, 22, 47, 19, 24, 14, 15, 32, 33, 1048, 6, 43, 41, 77, 8, 1049, 7, 1, 68, 22, 47, 19, 1050, 8, 435, 11, 1, 179, 1051, 176, 87, 16, 82, 62, 1052, 1053, 76, 1054, 1055, 22, 47, 19, 2, 8, 435, 1, 240, 22, 366, 8, 22, 17, 1, 1056, 5, 34, 1057, 1, 220, 66, 67, 34, 1058, 114, 340, 46, 2, 1059, 7, 22, 4, 414, 1060, 9, 34, 1061, 2, 166, 16, 231, 7, 6, 1, 165, 1062, 8, 141, 7, 61, 434, 86, 8, 1063, 117, 186, 11, 24, 14, 251, 163, 69, 7, 80, 104, 183, 7, 24, 14, 16, 69, 7, 2, 234, 26, 7, 104, 2, 43, 7, 112, 93, 81, 15, 1064, 11, 114, 233, 321, 8, 164, 269, 370, 4, 250, 5, 1, 13, 22, 206, 120, 2, 17, 34, 1065, 1066], [171, 40, 115, 20, 11, 96, 427, 29, 39, 88, 8, 78, 107, 41, 74, 11, 148, 46, 1067, 1068, 6, 399, 1069, 1070, 2, 107, 5, 42, 1071, 4, 245, 15, 1072, 45, 1073, 15, 96, 19, 91, 130, 10, 32, 5, 1, 19, 181, 8, 1074, 4, 393, 5, 12, 20, 35, 1, 238, 1075, 1076, 20, 1077, 1, 169, 149, 7, 2, 34, 259, 2, 8, 149, 7, 287, 109, 349, 90, 8, 1078, 1, 133, 17, 12, 20, 22, 216, 180, 21, 14, 232, 52, 7, 1, 20, 1079, 398, 1, 68, 432, 1080, 18, 1, 20, 1, 53, 10, 19, 2, 1, 88, 15, 39, 67, 107, 208, 1081, 1082, 17, 1, 88, 25, 431, 1083, 38, 53, 2, 175, 1084, 61, 246, 11, 271, 6, 1085, 1, 88, 62, 1086, 1087, 1088, 1089, 185, 52, 11, 175, 2, 53, 15, 421, 84, 1090, 221, 6, 1, 209, 162, 14, 1091, 38, 1, 158, 14, 52, 1, 158, 2, 15, 1092, 62, 1093, 30, 25, 15, 9, 1094, 3, 3, 150, 41, 74, 11, 1095, 1096, 51, 1097, 1098, 6, 1099, 6, 54, 8, 194, 91, 130, 6, 23, 32, 5, 1, 392, 46, 81, 1100, 12, 20, 46], [1101, 50, 1102, 1103, 310, 176, 2, 436, 1104, 1, 222, 6, 194, 1105, 2, 1106, 9, 4, 1107, 1108, 17, 4, 1109, 437, 1110, 1111, 15, 19, 84, 354, 103, 259, 6, 59, 46, 5, 76, 1112, 55, 1113, 76, 438, 176, 1114, 1115, 40, 1116, 1117, 2, 203, 1, 1118, 170, 1119, 227, 18, 76, 438, 21, 30, 1, 1120, 15, 1121, 25, 15, 248, 5, 215, 202, 2, 436, 430, 1122, 1, 1123, 1124, 5, 1125, 9, 50, 110, 437, 91, 130, 10, 4, 216, 1126, 115, 41, 347, 7, 1127, 29, 1128, 88, 2, 1129, 5, 1130, 1, 68, 10, 188, 19, 2, 1, 240, 10, 39, 1131, 46, 5, 142]]\n",
            "Encontrados 1131 unique tokens.\n",
            "Forma de data tensor: (19, 100)\n",
            "Forma de label tensor: (19,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.10 - Análisis del archivo de incrustaciones de palabras de GloVe\n",
        "glove_dir = 'sample_data/glove.6B'\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  #coefs = np.asarray(values[1:], dtype='float32')\n",
        "  #embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvI8o1emT0Wm",
        "outputId": "6ae4d214-e0e8-44ed-a886-2a8e95a5e00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listening 6.11 - PREPARACIÓN DE LA MATRIZ DE INTEGRACIÓN DE PALABRAS DEL GUANTE\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      print(embedding_matrix)"
      ],
      "metadata": {
        "id": "fAYkfTm_VESD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.12 - DEFINICION DE MODELO\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QODawszV9Y7",
        "outputId": "1b49e65f-b595-47f3-f2d0-eb3eeec0dd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 10000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.13 - Carga de incrustaciones de palabras preformadas en la capa de incrustación\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "DyXFIGwNWDnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.14 - ENTRENAMIENTO Y EVALUACION\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "epochs=10,\n",
        "batch_size=32,\n",
        "validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ijicty9WMQi",
        "outputId": "fb3abdab-0344-420d-e7a3-33f8c63e5183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 819ms/step - loss: 0.6931 - acc: 0.5263\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6931 - acc: 0.5263\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6930 - acc: 0.5263\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6930 - acc: 0.5263\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6929 - acc: 0.5263\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6929 - acc: 0.5263\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6929 - acc: 0.5263\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6928 - acc: 0.5263\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6928 - acc: 0.5263\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6928 - acc: 0.5263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.15 - Trazado de los resultados\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "c-juGNS4WbDj",
        "outputId": "557696fd-5637-4644-de1d-fe46fb086284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5zWc/rH8de76SQdVWspHaxGojHVVMgh50kosZJESGqdrUNYItsuy+62yNpCLJFzv6yQ4xY5NNFZkU7GYaWoCB1cvz8+n6m7aaa5q5m553A9H4953Pf9ub+H675jrvmcZWY455xzyaiS6gCcc86VH540nHPOJc2ThnPOuaR50nDOOZc0TxrOOeeS5knDOedc0jxpuJSR9KKkc4r72FSStETSMSVwXZO0T3x+n6Qbkzl2B+7TV9KkHY1zG9ftKim3uK/rSl/VVAfgyhdJ3ye8rAX8DGyMry80s7HJXsvMupXEsRWdmQ0qjutIagEsBqqZ2YZ47bFA0v+GrvLxpOG2i5nVznsuaQkwwMxezX+cpKp5v4iccxWHN0+5YpHX/CDpWklfAWMkNZD0H0nLJX0bnzdNOOdNSQPi8/6S3pJ0Zzx2saRuO3hsS0mTJa2R9KqkkZIeLSTuZGK8VdLb8XqTJDVKeL+fpKWSVki6YRvfT2dJX0lKSyg7RdKs+LyTpHckfSfpS0n3SKpeyLUekvTHhNdXx3O+kHRevmO7S/pQ0mpJn0m6OeHtyfHxO0nfSzo477tNOP8QSdMkrYqPhyT73WyLpP3i+d9Jmivp5IT3TpA0L17zc0lXxfJG8d/nO0krJU2R5L/DSpl/4a44/RrYDWgODCT89zUmvm4G/Ajcs43zOwMLgEbAX4AHJGkHjn0MeB9oCNwM9NvGPZOJ8UzgXOBXQHUg75dYG+Cf8fp7xvs1pQBm9h7wA3BUvus+Fp9vBK6In+dg4Gjgd9uImxhDdoznWKAVkL8/5QfgbKA+0B0YLKlnfO/w+FjfzGqb2Tv5rr0b8AJwV/xsfwNekNQw32fY6rspIuZqwPPApHjeJcBYSfvGQx4gNHXWAQ4AXo/lvwdygcbA7sD1gK+DVMo8abji9Asw1Mx+NrMfzWyFmT1jZmvNbA0wHDhiG+cvNbPRZrYReBjYg/DLIeljJTUDOgI3mdk6M3sLmFDYDZOMcYyZfWxmPwJPApmx/DTgP2Y22cx+Bm6M30FhHgf6AEiqA5wQyzCz6Wb2rpltMLMlwL8KiKMgp8f45pjZD4Qkmfj53jSz2Wb2i5nNivdL5roQkswnZvZIjOtxYD5wUsIxhX0323IQUBu4Lf4bvQ78h/jdAOuBNpLqmtm3ZvZBQvkeQHMzW29mU8wXzyt1njRccVpuZj/lvZBUS9K/YvPNakJzSP3EJpp8vsp7YmZr49Pa23nsnsDKhDKAzwoLOMkYv0p4vjYhpj0Trx1/aa8o7F6EWkUvSTWAXsAHZrY0xpEem16+inH8iVDrKMoWMQBL832+zpLeiM1vq4BBSV4379pL85UtBZokvC7suykyZjNLTLCJ1z2VkFCXSvqvpINj+R3AQmCSpEWShiT3MVxx8qThilP+v/p+D+wLdDazumxuDimsyak4fAnsJqlWQtle2zh+Z2L8MvHa8Z4NCzvYzOYRfjl2Y8umKQjNXPOBVjGO63ckBkITW6LHCDWtvcysHnBfwnWL+iv9C0KzXaJmwOdJxFXUdffK1x+x6bpmNs3MehCarsYTajCY2Roz+72Z7Q2cDFwp6eidjMVtJ08ariTVIfQRfBfbx4eW9A3jX+45wM2Sqse/Uk/axik7E+PTwImSDo2d1sMo+v+px4DLCMnpqXxxrAa+l9QaGJxkDE8C/SW1iUkrf/x1CDWvnyR1IiSrPMsJzWl7F3LtiUC6pDMlVZXUG2hDaEraGe8RaiXXSKomqSvh32hc/DfrK6mema0nfCe/AEg6UdI+se9qFaEfaFvNga4EeNJwJWkEsAvwDfAu8FIp3bcvoTN5BfBH4AnCfJKC7HCMZjYXuIiQCL4EviV01G5LXp/C62b2TUL5VYRf6GuA0THmZGJ4MX6G1wlNN6/nO+R3wDBJa4CbiH+1x3PXEvpw3o4jkg7Kd+0VwImE2tgK4BrgxHxxbzczW0dIEt0I3/u9wNlmNj8e0g9YEpvpBhH+PSF09L8KfA+8A9xrZm/sTCxu+8n7kVxFJ+kJYL6ZlXhNx7mKzmsarsKR1FHSbyRViUNSexDaxp1zO8lnhLuK6NfAs4RO6VxgsJl9mNqQnKsYvHnKOedc0rx5yjnnXNIqfPNUo0aNrEWLFqkOwznnyo3p06d/Y2aNC3qvwieNFi1akJOTk+ownHOu3JCUfyWATbx5yjnnXNI8aTjnnEuaJw3nnHNJq/B9Gs650rV+/Xpyc3P56aefij7YpVTNmjVp2rQp1apVS/qcpJJGnFX7DyANuN/MbivgmNMJa/kbMNPMzozltxPW5Qe41cyeiOUPAFmEFTc/Bvqb2feSDiespZMBnGFmTyfcYyMwO75cZmabdvtyzpUNubm51KlThxYtWlD4Hlou1cyMFStWkJubS8uWLZM+r8jmqbivwEjC4mJtgD5xx7LEY1oB1wFdzGx/4PJY3h1oT9iYpTNwlaS68bQrzOxAM8sAlgEXx/JlQH+2XDY6z49mlhl/SixhjB0LLVpAlSrhcezYkrqTcxXPTz/9RMOGDT1hlHGSaNiw4XbXCJPp0+gELDSzRXF1ynGEtXwSXQCMNLNvAczs61jeBpgcd/36AZgFZMdjVsfARVhl1GL5krjDWEqWPB47FgYOhKVLwSw8DhzoicO57eEJo3zYkX+nZJJGE7bcGSyXLXfuAkgnrLv/tqR3Y3MWwEwgO+6O1gg4ki03rRlD2PmrNXB3ErHUlJQT79GzsIMkDYzH5SxfvjyJy252ww2wdu2WZWvXhnLnnKvsimv0VFXCWvddCfv8jpZU38wmETZymUrYR+AdwsYpAJjZuYStHz8Ceidxn+ZmlkXYd2CEpN8UdJCZjTKzLDPLaty4wEmNhVq2bPvKnXNly4oVK8jMzCQzM5Nf//rXNGnSZNPrdevWbfPcnJwcLr300iLvccghhxRLrG+++SYnnnhisVyrtCSTND5ny+0km7L1do+5wIS42ftiQsd2KwAzGx77II5lc6f3Jma2kdDkdWpRgZhZ3naQi4A3gXZJxL9dmuXfLLOIcufczinuPsSGDRsyY8YMZsyYwaBBg7jiiis2va5evTobNmwo9NysrCzuuuuuIu8xderUnQuyHEsmaUwDWklqGbe0PIOw53Ci8YRaBrEZKh1YJClNUsNYnkEYETVJwT6xXIT9fuezDZIaSKqRcI8uwLykPuV2GD4catXasqxq1VDunCtepdWH2L9/fwYNGkTnzp255ppreP/99zn44INp164dhxxyCAsWLAC2/Mv/5ptv5rzzzqNr167svffeWyST2rVrbzq+a9eunHbaabRu3Zq+ffuSt3L4xIkTad26NR06dODSSy8tskaxcuVKevbsSUZGBgcddBCzZs0C4L///e+mmlK7du1Ys2YNX375JYcffjiZmZkccMABTJkypXi/sG0ocsitmW2QdDHwMmHI7YNmNlfSMCDHzCbE946TNI/Q/HS1ma2QVBOYEjtbVgNnxetVAR6OI6lE6PsYDGEDHeA5oAFwkqRb4ois/YB/SfqFkOxuM7NiTxp948aSN9wQmqRq14Y1a8JfQc654rWtPsS8/xeLS25uLlOnTiUtLY3Vq1czZcoUqlatyquvvsr111/PM888s9U58+fP54033mDNmjXsu+++DB48eKs5DR9++CFz585lzz33pEuXLrz99ttkZWVx4YUXMnnyZFq2bEmfPn2KjG/o0KG0a9eO8ePH8/rrr3P22WczY8YM7rzzTkaOHEmXLl34/vvvqVmzJqNGjeL444/nhhtuYOPGjazN/yWWoKTmaZjZRELfRGLZTQnPDbgy/iQe8xNhBFX+6/1CqCkUdK9phCaw/OVTgbbJxLuz+vbd/B/sunVw9NFw/vnQpg0ceGBpROBc5VCafYi//e1vSUtLA2DVqlWcc845fPLJJ0hi/fr1BZ7TvXt3atSoQY0aNfjVr37F//73P5o23fLXU6dOnTaVZWZmsmTJEmrXrs3ee++9af5Dnz59GDVq1Dbje+uttzYlrqOOOooVK1awevVqunTpwpVXXknfvn3p1asXTZs2pWPHjpx33nmsX7+enj17kpmZuVPfzfbwv5+LUL06PPUUNGgAp5wCK1emOiLnKo7S7EPcddddNz2/8cYbOfLII5kzZw7PP/98oXMVatSosel5Wlpagf0hyRyzM4YMGcL999/Pjz/+SJcuXZg/fz6HH344kydPpkmTJvTv359///vfxXrPbfGkkYRf/xqeeQZyc6FPH9i4sehznHNFK6gPsVatku9DXLVqFU2ahJkDDz30ULFff99992XRokUsWbIEgCeeeKLIcw477DDGxs6cN998k0aNGlG3bl0+/fRT2rZty7XXXkvHjh2ZP38+S5cuZffdd+eCCy5gwIABfPDBB8X+GQrjSSNJBx0EI0fCpEnwhz+kOhrnKoa+fWHUKGjeHKTwOGpU8fdn5HfNNddw3XXX0a5du2KvGQDssssu3HvvvWRnZ9OhQwfq1KlDvXr1tnnOzTffzPTp08nIyGDIkCE8/PDDAIwYMYIDDjiAjIwMqlWrRrdu3XjzzTc58MADadeuHU888QSXXXZZsX+GwlT4PcKzsrKsODdhuvDC8B/1U0/BaacV22WdqzA++ugj9ttvv1SHkXLff/89tWvXxsy46KKLaNWqFVdccUWqw9pKQf9ekqbHOXFb8ZrGdrrrrlDr6N8f5s5NdTTOubJq9OjRZGZmsv/++7Nq1SouvPDCVIdULLymsQO++ALat4c6dWDaNKhfv1gv71y55jWN8sVrGqVgzz3h6adhyZLQ9vpLSpZWdM650udJYwcdeij84x8wcSLcckuqo3HOudLhSWMnDB4M554Lw4bB//1fqqNxzrmS50ljJ0hw772QlQX9+sH8ba6e5Zxz5Z8njZ1UsyY8+2x47NkTVq9OdUTOVW5HHnkkL7/88hZlI0aMYPDgwYWe07VrV/IGzJxwwgl89913Wx1z8803c+edd27z3uPHj2fevM1L4t100028+uqr2xN+gcrSEuqeNIrBXnvBk0/CwoVw9tneMe5cKvXp04dx48ZtUTZu3LikFg2EsDpt/R0cEpk/aQwbNoxjjjlmh65VVnnSKCZdu8Jf/xr6Nv70p1RH41zlddppp/HCCy9s2nBpyZIlfPHFFxx22GEMHjyYrKws9t9/f4YOHVrg+S1atOCbb74BYPjw4aSnp3PooYduWj4dwhyMjh07cuCBB3Lqqaeydu1apk6dyoQJE7j66qvJzMzk008/pX///jz99NMAvPbaa7Rr1462bdty3nnn8fPPP2+639ChQ2nfvj1t27ZlfhHt3KleQj2pVW5dci69FHJy4KaboF076N491RE5l1qXXw4zZhTvNTMzYcSIwt/fbbfd6NSpEy+++CI9evRg3LhxnH766Uhi+PDh7LbbbmzcuJGjjz6aWbNmkZGRUeB1pk+fzrhx45gxYwYbNmygffv2dOjQAYBevXpxwQUXAPCHP/yBBx54gEsuuYSTTz6ZE088kdPyLRfx008/0b9/f1577TXS09M5++yz+ec//8nll18OQKNGjfjggw+49957ufPOO7n//vsL/XypXkLdaxrFSIJ//Sssn963L3zySaojcq5ySmyiSmyaevLJJ2nfvj3t2rVj7ty5WzQl5TdlyhROOeUUatWqRd26dTn55JM3vTdnzhwOO+ww2rZty9ixY5lbxPIQCxYsoGXLlqSnpwNwzjnnMHny5E3v9+rVC4AOHTpsWuSwMG+99Rb9+vUDCl5C/a677uK7776jatWqdOzYkTFjxnDzzTcze/Zs6tSps81rJ8NrGsWsVi147jno0CF0jL/3XtjIybnKaFs1gpLUo0cPrrjiCj744APWrl1Lhw4dWLx4MXfeeSfTpk2jQYMG9O/fv9Al0YvSv39/xo8fz4EHHshDDz3Em2++uVPx5i2vvjNLqw8ZMoTu3bszceJEunTpwssvv7xpCfUXXniB/v37c+WVV3L22WfvVKxe0ygBLVrAE0+EIbjnnhu2sXTOlZ7atWtz5JFHct55522qZaxevZpdd92VevXq8b///Y8XX3xxm9c4/PDDGT9+PD/++CNr1qzh+eef3/TemjVr2GOPPVi/fv2m5cwB6tSpw5o1a7a61r777suSJUtYuHAhAI888ghHHHHEDn22VC+hnlTSkJQtaYGkhZKGFHLM6ZLmSZor6bGE8tslzYk/vRPKH5A0U9IsSU9Lqh3LD5f0gaQNkk7Ld49zJH0Sf87ZsY9cOo45Bm6/PSw38pe/pDoa5yqfPn36MHPmzE1JI28p8datW3PmmWfSpUuBm4du0r59e3r37s2BBx5It27d6Nix46b3br31Vjp37kyXLl1o3br1pvIzzjiDO+64g3bt2vHpp59uKq9ZsyZjxozht7/9LW3btqVKlSoMGjRohz5XypdQN7Nt/hD2Bf8U2BuoTtjPu02+Y1oBHwIN4utfxcfuwCuEZrBdgWlA3fhe3YTz/wYMic9bABnAv4HTEo7ZDVgUHxvE5w2Kir9Dhw6WKr/8Yta7t5lk9tJLKQvDuVI1b968VIfgtkNB/15AjhXyOzWZmkYnYKGZLTKzdcA4oEe+Yy4ARprZtzERfR3L2wCTzWyDmf0AzAKy4zGrASQJ2AWwWL7EzGYB+Wc7HA+8YmYr431eybtWWSXBAw/AAQeEHf8WLUp1RM45t3OSSRpNgM8SXufGskTpQLqktyW9Kynvl/lMIFtSLUmNgCOBvfJOkjQG+ApoDdxdDHHkXXegpBxJOcuXLy/isiVr111Dx7hZ2GP8hx9SGo5zzu2U4uoIr0poouoK9AFGS6pvZpOAicBU4HHgHWDTDttmdi6wJ/AR0JtiYmajzCzLzLIaN25cXJfdYb/5DTz+OMyeDRdc4B3jruIz/4+8XNiRf6dkksbnJNQOgKaxLFEuMMHM1pvZYuBjQhLBzIabWaaZHQsovpcY9EZCk9epxRBHmZWdDX/8Y0gef/97qqNxruTUrFmTFStWeOIo48yMFStWULNmze06L5l5GtOAVpJaEn5JnwGcme+Y8YQaxpjYDJUOLJKUBtQ3sxWSMggd3JNiP8ZvzGxhfH4yUNQasS8Df5LUIL4+DrguifjLjOuug+nT4eqrw6zWo45KdUTOFb+mTZuSm5tLqpuGXdFq1qxJ06ZNt+ucIpOGmW2QdDHhl3Ya8KCZzZU0jNDDPiG+d5ykeYTmp6tjoqgJTAl5gdXAWfF6VYCHJdUl1D5mAoMBJHUEniOMkDpJ0i1mtr+ZrZR0KyGJAQwzs5Xb9WlTTIKHHoLOneH000MCad481VE5V7yqVatGy5YtUx2GKyG+R3gKfPwxdOwI++wDb70Fu+yS6oicc24z3yO8jElPh7Fj4YMPYNAg7xh3zpUfnjRS5MQT4eab4d//hnvuSXU0zjmXHE8aKXTjjXDSSXDFFZCw4KVzzpVZnjRSqEoVeOSRMI/jt7+F3NxUR+Scc9vmSSPF6tWD8eNh7Vo49VTYwZWanXOuVHjSKAP22y/0bbz/Plx0kXeMO+fKLk8aZcQpp8ANN8CDD4bd/5xzrizypFGG3HILdOsW9hqfOjXV0Tjn3NY8aZQhaWlh/kazZqF/44svUh2Rc85tyZNGGdOgQegYX70aunYNy4xUqRK2kE3YVdI551LCk0YZdMABcN558MknsGxZ6BhfuhQGDvTE4ZxLLU8aZVTCHvabrF0bOsudcy5VPGmUUcuWbV+5c86VBk8aZVSzZgWXb+fS9845V6w8aZRRw4dDrVpbl0vwebnZr9A5V9F40iij+vaFUaPC6CkpPF5zDaxcCQcdFPYbd8650pZU0pCULWmBpIWShhRyzOmS5kmaK+mxhPLbJc2JP70Tyh+QNFPSLElPS6ody2tIeiLe6z1JLWJ5C0k/SpoRf+7bmQ9eHvTtC0uWwC+/hMfbb4cpU2DjRjj0UHj99VRH6JyrbIpMGnGf75FAN6AN0EdSm3zHtCLs193FzPYHLo/l3YH2QCbQGbgqbvEKcIWZHWhmGcAy4OJYfj7wrZntA/wduD3hVp+aWWb8GbRDn7icy8yEd9+FvfaC7OywSq5zzpWWZGoanYCFZrbIzNYB44Ae+Y65ABhpZt8CmNnXsbwNMNnMNpjZD8AsIDsesxpAYQPxXYC8Zfp6AA/H508DR8djXNSsWdgm9rDD4Oyz4Y9/9EUOnXOlI5mk0QT4LOF1bixLlA6kS3pb0ruSsmP5TCBbUi1JjYAjgb3yTpI0BvgKaA3cnf9+ZrYBWAU0jO+1lPShpP9KOizZD1kR1a8PL74I/fqFzZwuuADWr091VM65iq5qMV6nFdAVaApMltTWzCZJ6ghMBZYD7wAb804ys3Nj89fdQG9gzDbu8SXQzMxWSOoAjJe0f16NJZGkgcBAgGaFjV2tAKpXh4cfDkuM3Hpr2MTpqaegTp1UR+acq6iSqWl8TkLtgJAU8g/6zAUmmNl6M1sMfExIIpjZ8NgHcSyg+N4mZraR0OR1av77SaoK1ANWmNnPZrYinjMd+JRQw9mKmY0ysywzy2rcuHESH7H8kmDYMLj/fnj1VTj8cF/o0DlXcpJJGtOAVpJaSqoOnAFMyHfMeEItg9gMlQ4skpQmqWEszwAygEkK9onlAk4G5sdrTQDOic9PA143M5PUONZKkLQ3ISkt2oHPXCGdfz688AIsXBiG5M6Zk+qInHMVUZFJI/YrXAy8DHwEPGlmcyUNk3RyPOxlYIWkecAbwNWxVlANmBLLRwFnxesJeFjSbGA2sAcwLF7rAaChpIXAlUDeEN/DgVmSZhA6yAeZ2cqd/PwVyvHHw+TJsGGDD8l1zpUMWQUfdpOVlWU5OTmpDqNULVsGJ5wAH38cdgI866xUR+ScK08kTTezrILe8xnhFVDekNxDDw2jq4YP9yG5zrni4UmjgqpfH156KdQy/vCHsBeHD8l1zu2s4hpy68qg6tXh3/8OQ3L/+McwJPfJJ31IrnNux3lNo4KTwhyO0aPhlVd8SK5zbud40qgkBgyA//xn85DcuXNTHZFzrjzypFGJZGdvHpLbpQu88UaqI3LOlTeeNCqZdu3CKrlNm4Z5HY8+muqInHPliSeNSsiH5DrndpQnjUoq/5DcCy8MzVbOObctPuS2Essbktu8eahtfPaZD8l1zm2b1zQqOSnM4Rg1KgzJPeIIH5LrnCucJw0HhE2cnn8ePvkEDj7Yh+Q65wrmScNt0q1bGJK7bp0PyXXOFcyThttC3pDcJk3CkNzf/S4sQ1KlSngcOzbVETrnUsk7wt1WmjeHt9+GQw6Bf/5zc/nSpWHhQ4C+fVMTm3Mutbym4QpUvz788MPW5WvXwg03lH48zrmywZOGK9RnnxVcvmxZ6cbhnCs7kkoakrIlLZC0UNKQQo45XdI8SXMlPZZQfrukOfGnd0L5A5JmSpol6WlJtWN5DUlPxHu9J6lFwjnXxfIFko7f0Q/tktOsWcHlDRr4DHLnKqsik4akNGAk0A1oA/SR1CbfMa2A64AuZrY/cHks7w60BzKBzsBVkurG064wswPNLANYRtiHHOB84Fsz2wf4O3B7vFYb4AxgfyAbuDfG5krI8OFQq9aWZVWqwMqV0LMnLF+emricc6mTTE2jE7DQzBaZ2TpgHNAj3zEXACPN7FsAM/s6lrcBJpvZBjP7AZhF+IWPma0GkCRgFyDvb9cewMPx+dPA0fGYHsA4M/vZzBYDC2NsroT07Rsm/TVvHiYBNm8ODz0Ef/97WIIkIwMmTUp1lM650pRM0mgCJLZu58ayROlAuqS3Jb0rKTuWzwSyJdWS1Ag4Etgr7yRJY4CvgNbA3fnvZ2YbgFVAwyTjcMWsb19YsgR++SU89usHl18O06ZBw4ZhWO4VV8BPP6U6UudcaSiujvCqQCugK9AHGC2pvplNAiYCU4HHgXeAjXknmdm5wJ7AR0BviomkgZJyJOUs9zaUEpGRERLHpZfCiBHQqRPMmZPqqJxzJS2ZpPE5CbUDoGksS5QLTDCz9bHp6GNCEsHMhptZppkdCyi+t4mZbSQ0eZ2a/36SqgL1gBVJxpF3zVFmlmVmWY0bN07iI7odscsu8I9/wMSJ8PXXkJUFd9/tneTOVWTJJI1pQCtJLSVVJ3RGT8h3zHhCLYPYDJUOLJKUJqlhLM8AMoBJCvaJ5QJOBubHa00AzonPTwNeNzOL5WfE0VUtCUnp/R34zK6YdesGs2bBsceGmscJJ8BXX6U6KudcSShyRriZbZB0MfAykAY8aGZzJQ0DcsxsQnzvOEnzCM1PV5vZCkk1gSkhL7AaOCterwrwcBxJJULfx+B4yweARyQtBFYSkhTxnk8C84ANwEWxluLKgF/9CiZMgPvugyuvDM1XDz4IJ56Y6sicc8VJVsHbErKysiwnJyfVYVQq8+bBmWfCzJlh7ao77th66K5zruySNN3Msgp6z2eEu2LXpg289x78/vdw772hr2PGjFRH5ZwrDp40XImoUQPuvDNs7PTdd2F01V//GobuOufKL08arkQdcwzMnh36Nq66Co47Dj4vcMybc6488KThSlzDhvDMMzB6NLzzTugkf+65VEflnNsRnjRcqZBgwAD48EPYe2/o1StsMfv996mOzDm3PTxpuFKVng5Tp8L118MDD0D79mFmuXOufPCk4UpdtWphBd033ghrVh1yCPz5z7DRZ904V+Z50nApc8QRYS7HqaeGmsdRR/kGT86VdZ40XEo1aACPPw7//nfo78jIgCeeSHVUzrnCeNJwKSeFJddnzAgTA884A845B1avTnVkzrn8PGm4MmPvvWHyZBg6FB59FDIzwxBd51zZ4UnDlSlVq8LNN8OUKWGJ9cMOg1tuCc1XLVqE7WZbtICxY1McqHOVVJGr3DqXCoccEjrJL744JJEqVTYvQbJ0KQwcGOR4HMYAABpGSURBVJ737ZuyEJ2rlLym4cqsunVDDaNRo63XrFq7Fm64ITVxOVeZedJwZd6KFQWX+/Bc50qfJw1X5jVrVnB53bphcqBzrvR40nBl3vDhW2/ilJYGq1ZB27YwaVJq4nKuMkoqaUjKlrRA0kJJQwo55nRJ8yTNlfRYQvntkubEn94J5WPjNedIelBStVjeQNJzkmZJel/SAQnnLJE0W9IMSb4dXyXRty+MGgXNm4c5Hc2bw8MPw6uvhtfHHx/mdnz5Zaojda4SMLNt/hD2Bf8U2BuoTtjPu02+Y1oBHwIN4utfxcfuwCuEUVq7AtOAuvG9Ewj7gwt4HBgcy+8AhsbnrYHXEu6zBGhUVMyJPx06dDBXcf34o9ktt5jVqGFWt67Z3XebbdiQ6qicK9+AHCvkd2oyNY1OwEIzW2Rm64BxQI98x1wAjDSzb2Mi+jqWtwEmm9kGM/sBmAVkx2MmJgT4PtA04ZzX4zHzgRaSdk8iTlcJ1awJN90Ec+bAQQfBJZdA587g28I7VzKSSRpNgM8SXufGskTpQLqktyW9Kyk7ls8EsiXVktQIOBLYK/HE2CzVD3gp4Zxe8b1OQHM2JxQDJkmaLmlgYQFLGigpR1LO8uXLk/iIrrzbZx946SUYNy7sDNipU0ggq1alOjLnKpbi6givSmii6gr0AUZLqm9mk4CJwFRCE9Q7QP4FsO8l1EamxNe3AfUlzQAuITR75Z1zqJm1B7oBF0k6vKBgzGyUmWWZWVbjxo2L6SO6sk6C3r1h/vwwKfDee6F165BIQuumc25nJZM0PmfL2kHTWJYoF5hgZuvNbDHwMSGJYGbDzSzTzI4l9F98nHeSpKFAY+DKvDIzW21m55pZJnB2fH9RfO/z+Pg18Byh6cy5LdSrB3fdBe+/D02bQp8+YW/yTz5JdWTOlX/JJI1pQCtJLSVVB84AJuQ7ZjyhlkFshkoHFklKk9QwlmcAGcCk+HoAcDzQx8w2zfeVVD/eB2AAoRayWtKukurEY3YFjgPm7MBndpVEhw7w7rtwzz0hgbRtG9ax8rkdzu24IpOGmW0ALgZeBj4CnjSzuZKGSTo5HvYysELSPOAN4GozWwFUA6bE8lHAWfF6APcBuwPvxCG0N8Xy/YA5khYQmqEui+W7A29JmknoOH/BzPL6QZwrUFoaXHRRaLI65ZSwjlXbtvDKK6mOzLnySVbBG3uzsrIsx4fSuOiVV+B3v4OFC0Oz1d/+Br/+daqjcq5skTTdzLIKes9nhLtK5dhjYfbsUON45hnYd18YOdL3J3cuWZ40XKVTs2bY6Gn27DA09+KLwxyP6dNTHZlzZZ8nDVdppaeHdasefxxyc0MCufRSn9vh3LZ40nCVmhTWrfroIxg8OIy02m8/eOIJn9vhXEE8aTgH1K8fEsZ778Eee4REkp0dOsydc5t50nAuQceOYU7HXXfBO+/AAQfAsGHw88+pjsy5ssGThnP5pKWFdavmz4eePUOneUYGvPYajB0LLVqEPctbtAivnatMPGk4V4g99wzrVr38chiSe8wxcM45sHRp6O9YuhQGDvTE4SoXTxrOFeG448Lw3Hr1tp7PsXYt3HBDauJyLhU8aTiXhF12gdWrC35v6dLSjcW5VPKk4VySmjUruLxGjbAwonOVgScN55I0fDjUqrVlWbVqYYb5wQfD6afDp5+mJjbnSosnDeeS1LcvjBoFzZuHSYHNm8OYMfDZZ2HL2RdeCBMDL78cVqxIdbTOlQxf5da5YvLFF2F47oMPQp06cP31YVmSmjVTHZlz28dXuXWuFOy5J4weDTNnQpcucO21YRXdsWPhl1+KPt+58sCThnPF7IADQlPVa69Bw4Zw1llhpvkbb6Q6Mud2nicN50rIUUdBTg488gh88014feKJMG9eqiNzbscllTQkZUtaIGmhpCGFHHO6pHmS5kp6LKH8dklz4k/vhPKx8ZpzJD0oqVosbyDpOUmzJL0v6YDticO5sqRKlVDTWLAAbr8d3norbDc7cCB8+WWqo3Nu+xWZNCSlASMJ+3W3AfpIapPvmFbAdUAXM9sfuDyWdwfaA5lAZ+AqSXXjaWOB1kBbYBdgQCy/HphhZhnA2cA/ko3DubKqZk245pqwau4ll4RRV61ahR0Ev/8+1dE5l7xkahqdgIVmtsjM1gHjgB75jrkAGGlm3wKY2dexvA0w2cw2mNkPwCwgOx4z0SLgfaBpwjmvx2PmAy0k7Z5kHM6VaY0awYgRYf+Obt3glltC8hg1CjZsSHV0zhUtmaTRBPgs4XVuLEuUDqRLelvSu5KyY/lMIFtSLUmNgCOBvRJPjM1S/YCXEs7pFd/rBDQnJJRk4si75kBJOZJyli9fnsRHdK507bMPPPUUTJ0Ke+8NF14IBx4I//mPb/7kyrbi6givCrQCugJ9gNGS6pvZJGAiMBV4HHgHyLfkG/cSaiNT4uvbgPqSZgCXAB8WcM42mdkoM8sys6zGjRvv4EdyruQdfHDo53jmGVi/Hk46KXSY+37lrqxKJml8zpa1g6axLFEuMMHM1pvZYuBjQhLBzIabWaaZHQsovgeApKFAY+DKvDIzW21m55pZJqFPozGwKMk4nCt3JOjVC+bOhbvvhjlzICsrzEBfsiTV0Tm3pWSSxjSglaSWkqoDZwAT8h0znlDLIDZDpQOLJKVJahjLM4AMYFJ8PQA4HuhjZpumPkmqH+8DoXN8spmtTjIO58qtatXg4otDZ/l118Gzz4bJgVdfDd9+m+ronAuKTBpmtgG4GHgZ+Ah40szmShom6eR42MvACknzgDeAq81sBVANmBLLRwFnxesB3AfsDrwjaYakm2L5fsAcSQsII6Uu21YcO/n5nStz6tWDP/0JPv4Y+vSBv/419IGMGAHr1qU6OlfZ+dpTzpVxM2aE4bqvvBI6zf/855A8/vAHWLYsLNk+fHhoznKuOGxr7amqpR2Mc277ZGbCpElh29mrr4bevcOkwbz1rPK2nQVPHK7k+TIizpUTxx8PH34Y1rPKvwCibzvrSosnDefKkbQ0WLmy4Pd821lXGjxpOFfOFLbtLITdA+f68BBXgjxpOFfOFLTt7C67QM+e8NJLYUHEPn1g/vzUxOcqNk8azpUzBW07O3o0PPccLF4MQ4bA88/D/vtDv37wySepjthVJD7k1rkKaPlyuOMOuOce+PlnOPtsuPHGMGTXuaL4dq/OVTKNG8Nf/hJqHpddBuPGQXo6DBjgS5O4neNJw7kKbPfd4W9/g0WL4KKL4NFHw1LsgwaFiYHObS9PGs5VAnvsAf/4R1jXauBAePDBkDwuugg+92U/3XbwpOFcJdK0KYwcGZLHueeGDvXf/CY0Yfn2sy4ZnjScq4SaNYP77gsjq846KySSvfeG3/8e/ve/VEfnyjJPGs5VYi1awP33w4IFYU2rESNC8rj2Wvjmm1RH58oiTxrOOX7zG3joobB3ea9eYbhuixZw/fWwYkWqo3NliScN59wm6enwyCNhKZKTToLbboOWLeGmm3wjKBd40nDObWW//eDxx2H2bMjOhltvDclj2DBYtSrV0blU8qThnCvU/vvDk0/CzJlw1FEwdGhIHsOHw5o1MHZsaMaqUiU8jh2b6ohdSUsqaUjKlrRA0kJJQwo55nRJ8yTNlfRYQvntkubEn94J5WPjNedIelBStVheT9LzkmbGa52bcM7GuDXsDEm+P7hzpSQjI+xZPn06HHpo2DVwzz3DsN2lS8Fs82ZQnjgqtiLXnpKUBnwMHAvkAtOAPmY2L+GYVsCTwFFm9q2kX5nZ15K6A5cT9vquAbwJHG1mqyWdALwYL/EYMNnM/inpeqCemV0rqTGwAPi1ma2T9L2Z1d6eD+hrTzlX/KZNg8MPh59+2vq95s19qZLybmfXnuoELDSzRWa2DhgH9Mh3zAXASDP7FsDMvo7lbQjJYIOZ/QDMArLjMRMtAt4HmsZzDKgjSUBtYCWwIcnP6pwrBR07hoUQC+KbQVVsySSNJsBnCa9zY1midCBd0tuS3pWUHctnAtmSaklqBBwJ7JV4YmyW6ge8FIvuAfYDvgBmA5eZWd7mljUl5cR79CwsYEkD43E5y5cvT+IjOue217Y2gxo8GD79tPRicaWnuDrCqwKtgK5AH2C0pPpmNgmYCEwFHgfeATbmO/deQm1kSnx9PDAD2BPIBO6RVDe+1zxWmc4ERkj6TUHBmNkoM8sys6zGjRsX00d0ziUqaDOomjXhyCPD2lbp6XDmmaET3VUcySSNz9mydtA0liXKBSaY2XozW0zoA2kFYGbDzSzTzI4FFN8DQNJQoDFwZcK1zgWejS1XC4HFQOt4rc/j4yJC/0i7JD+nc66YFbQZ1P33w+uvhz6Nq66C//wHMjPhhBNg8uTQYe7Kt2SSxjSglaSWkqoDZwD5Ry6NJ9QyiM1Q6cAiSWmSGsbyDCADmBRfDyDUKvokND8BLAOOjsfsDuwbr9VAUo2Ee3QB5uGcS5m+fUOC+OWX8Ni3byjfYw+4/faw/Prw4ZCTA0ccAV26wIQJ4XhXPhWZNMxsA3Ax8DLwEfCkmc2VNEzSyfGwl4EVkuYBbwBXm9kKoBowJZaPAs6K1wO4D9gdeCcOob0plt8KHCJpNvAacK2ZfUPo58iRNDPe47bEEVzOubKnfv2wFMnSpWFRxC+/hB49whDeRx6B9etTHaHbXr7dq3Ou1GzYAE88EZYnmTMndKZfdRWcf/7W/SMudXy7V+dcmVC1amjCmjUr9HfstRdcemnoD/njH319q/LAk4ZzrtRJ0L07vPUWTJkCnTvDjTdurnl88UWqI3SF8aThnEupQw8NtY6ZM0N/x4gRYX2rCy6Ajz8u+nxXujxpOOfKhIwMePTRsJvggAHheevW8NvfhjWvXNngScM5V6a0bBlGWi1ZAtddB6+8AllZcNxxYQ5IBR+7U+Z50nDOlUm77x7meCxbFuZ8zJ4NRx8NBx0Ezz3ncz1SxZOGc65Mq1sXrrkGFi+Gf/0rbD/bqxe0aQNjxsC6db6vR2nyeRrOuXJl40Z4+ukw12PGDNhtt7AhVOJEwVq1whIneTPU3fbxeRrOuQojLQ1694YPPoCXXoIffth6ZvnatXDDDamJr6LzpOGcK5ckOP740DxVEN/Xo2R40nDOlWvb2tfj8MPDNrUb82/I4HaYJw3nXLlW0L4eu+wS+jM++wxOPRVatQqTBlevTk2MFYknDedcuVbQvh6jR4fJgQsXwjPPQJMmcMUV0LRpeFy8ONVRl18+eso5Vynk5MDf/w5PPhnmePTsGRJIly4h2bjNfPSUc67Sy8oK8zeWLIFrr4U334TDDoNOnUJ5YR3qbkueNJxzlUqTJvCnP4X+jn/+M8zxOOussHzJn/8cJg+6wnnScM5VSrVqwaBBMG8eTJwI++8fdhnca69QPn9+qiMsm5JKGpKyJS2QtFDSkEKOOV3SPElzJT2WUH67pDnxp3dC+dh4zTmSHpRULZbXk/S8pJnxWucmnHOOpE/izzk7/rGdcy6oUgW6dYNJk8L6Vn37wkMPwX77wQknhPIK3vW7XYpMGpLSgJFAN6AN0EdSm3zHtAKuA7qY2f7A5bG8O9AeyAQ6A1dJqhtPGwu0BtoCuwADYvlFwDwzOxDoCvxVUnVJuwFD43U6AUMlNdjBz+2cc1s54IAw8uqzz2DYsDDr/PjjoW1buP9++PHHVEeYesnUNDoBC81skZmtA8YBPfIdcwEw0sy+BTCzr2N5G2CymW0wsx+AWUB2PGaiRcD7QNN4jgF1JAmoDawENgDHA6+Y2cp4n1fyruWcc8WpceOwk+DSpaHWUbVq2BSqWbNQ/uWXqY4wdZJJGk2AzxJe58ayROlAuqS3Jb0rKe+X+UwgW1ItSY2AI4G9Ek+MzVL9gJdi0T3AfsAXwGzgMjP7Jck48q45UFKOpJzly5cn8RGdc25rNWrAOefAhx/CG2/AIYeEyYTNm4fyGTNSHWHpK66O8KpAK0JzUh9gtKT6ZjYJmAhMBR4H3gHyT+i/l1AbmRJfHw/MAPYkNGvdk9CklRQzG2VmWWaW1bhx4x38SM45F0jQtSv83/+FLWgHDQqTBtu121z+yCOVY3n2ZJLG52xZO2gayxLlAhPMbL2ZLQY+JiQRzGy4mWWa2bGA4nsASBoKNAauTLjWucCzseVqIbCY0PeRTBzOOVei9tkH7roLcnPhjjvC7PKePUPNY+nS0Gm+dCkMHFgxE0cySWMa0EpSS0nVgTOACfmOGU+oZRCbodKBRZLSJDWM5RlABjApvh5AqFX0ic1PeZYBR8djdgf2BRYBLwPHSWoQO8CPi2XOOVfq6teHq66CTz+FRo22HmFVUZdnr1rUAWa2QdLFhF/QacCDZjZX0jAgx8wmsPkX+jxC89PVZrZCUk1gSujTZjVwlpltiJe+D1gKvBPff9bMhgG3Ag9Jmk2omVxrZt8ASLqVkMQAhpnZymL4DpxzbodVrVr4hMClS0PT1Yknhn1AKgJfe8o553ZSixYF79+RlhaWZW/WLPSDDBgQRmaVdb72lHPOlaCClmevVQsefDB0mO+zT5ht3rQp9OsH775bficMetJwzrmdVNDy7KNGwdlnQ69e8NprYbmSgQNDc9XBB0PHjjBmTPmbMOjNU845V4rWrAl7fYwcCXPnwm67wXnnweDBsPfeqY4u8OYp55wrI+rUCQli9uwwYfCoo8I+H/vsA927h8UTf/ml6OukiicN55xLgbwJg089FTrRb7wxrHXVvXvYnvavf4WVZXB8qCcN55xLsSZN4JZbQvIYNy68vuqq8Hj++SGZlBWeNJxzroyoXh1694bJk2HmzNCRPm4cdOgQOs8ffRR+/jm1MXrScM65MigjA/71L/j8cxgxIjRV9esXNom64QZYtiw1cXnScM65Mqx+fbjsMvjoo7Ah1CGHwG23he1pTzkFXn21dOd8eNJwzrlyoEoVOPZYGD8eFi2Ca66Bt94KZfvtB3ffDatWhUUSS3K1XZ+n4Zxz5dRPP4XRVyNHwnvvhf0/Nm6EDRs2H1OrVpho2Ldv8tf1eRrOOVcB1ay5eVmSnJyweGJiwoDiX23Xk4ZzzlUAHTqEBFGQ4uw096ThnHMVRLNm21e+IzxpOOdcBVHYarvDhxffPTxpOOdcBVHYarvb0wlelCJ37nPOOVd+9O1bvEkiv6RqGpKyJS2QtFDSkEKOOV3SPElzJT2WUH67pDnxp3dC+dh4zTmSHpRULZZfLWlG/JkjaaOk3eJ7SyTNju/5OFrnnCtlRSYNSWnASKAb0AboI6lNvmNaAdcBXcxsf+DyWN4daA9kAp2BqyTVjaeNBVoDbYFdgAEAZnaHmWWaWWa85n/z7QV+ZHy/wDHEzjnnSk4yNY1OwEIzW2Rm64BxQI98x1wAjDSzbwHM7OtY3gaYbGYbzOwHYBaQHY+ZaBHwPtC0gHv3AR7f3g/lnHOuZCSTNJoAnyW8zo1lidKBdElvS3pXUnYsnwlkS6olqRFwJLBX4omxWaof8FK+8lqEBPNMQrEBkyRNlzSwsIAlDZSUIyln+fLlSXxE55xzySiujvCqQCugK6HGMFlSWzObJKkjMBVYDrwDbMx37r2E2siUfOUnAW/na5o61Mw+l/Qr4BVJ881scv5gzGwUMArCMiI7//Gcc85Bcknjc7asHTSNZYlygffMbD2wWNLHhCQyzcyGA8MBYgf5x3knSRoKNAYuLOC+Z5CvacrMPo+PX0t6jtB0tlXSSDR9+vRvJC0t6kOWcY2Ab1IdRBnh38WW/PvYkn8fm+3Md9G8sDeSSRrTgFaSWhKSxRnAmfmOGU/ofxgTm6HSgUWxE72+ma2QlAFkAJMAJA0AjgeONrMtdsSVVA84AjgroWxXoIqZrYnPjwOGFRW8mTVO4jOWaZJyvOM/8O9iS/59bMm/j81K6rsoMmmY2QZJFwMvA2nAg2Y2V9IwIMfMJsT3jpM0j9D8dHVMFDWBKZIAVgNnmVneclr3AUuBd+L7z5pZXhI4BZgUO8/z7A48F4+tCjxmZlv0gzjnnCtZFX5p9IrA/3razL+LLfn3sSX/PjYrqe/ClxEpH0alOoAyxL+LLfn3sSX/PjYrke/CaxrOOeeS5jUN55xzSfOk4ZxzLmmeNMooSXtJeiNhEcjLUh1TWSApTdKHkv6T6lhSSVJ9SU9Lmi/pI0kHpzqmVJJ0Rfz/ZI6kx+PIzUojLvr6taQ5CWW7SXpF0ifxsUFx3MuTRtm1Afi9mbUBDgIuyr9QZCV1GfBRqoMoA/4BvGRmrYEDqcTfiaQmwKVAlpkdQJgacEZqoyp1DxHX9UswBHjNzFoBr8XXO82TRhllZl+a2Qfx+RrCL4X8a35VKpKaAt2B+1MdSyrFya+HAw8AmNk6M/sutVGlXFVgF0lVgVrAFymOp1TF5ZRW5ivuATwcnz8M9CyOe3nSKAcktQDaAe+lNpKUGwFcA/xS1IEVXEvCWm5jYlPd/XGVhEopLi90J7AM+BJYZWaTUhtVmbC7mX0Zn39FmCC90zxplHGSahNW+r3czFanOp5UkXQi8LWZTU91LGVAVcI+Nf80s3bADxRT00N5FNvqexCS6Z7ArpLO2vZZlUvcgqJY5ld40ijD4rLxzwBjzezZVMeTYl2AkyUtIezpcpSkR1MbUsrkArlmllfzfJqQRCqrY4DFZrY8Lpr6LHBIimMqC/4naQ+A+Ph1EccnxZNGGaWwyNYDwEdm9rdUx5NqZnadmTU1sxaETs7XzaxS/jVpZl8Bn0naNxYdDcxLYUiptgw4KO7bI8L3UWkHBiSYAJwTn58D/F9xXNSTRtnVhbA51VEJe6afkOqgXJlxCTBW0izCdsp/SnE8KRNrXE8DHwCzCb/XKtVyIpIeJ+xXtK+kXEnnA7cBx0r6hFAbu61Y7uXLiDjnnEuW1zScc84lzZOGc865pHnScM45lzRPGs4555LmScM551zSPGk455xLmicN55xzSft/DJ313CQ5zXIAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.16 - Entrenamiento del mismo modelo sin incrustaciones de palabras pretratadas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['acc'])\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBm3eW_Ccw8G",
        "outputId": "2a8705e2-1233-4a8e-c7d7-946b7fbcafa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 578ms/step - loss: 0.6909 - acc: 0.5789\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4410 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2873 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1869 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1164 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0738 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0503 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0365 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0277 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0218 - acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.17 - Tokenización de los datos del conjunto de pruebas\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(test_dir, label_type)\n",
        "  for fname in sorted(os.listdir(dir_name)):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)\n",
        "print(x_test)\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "kKNsbI1VfLmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d01255d-3fb8-42db-f2ed-b386f72de06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   3    3   21 ...   20  740  129]\n",
            " [  23    6   29 ... 1049   12   20]\n",
            " [   0    0    0 ...    1    5  683]\n",
            " ...\n",
            " [   0    0    0 ...   25   98    6]\n",
            " [  49   44   27 ...   10   35  210]\n",
            " [   8  149    1 ...  117  432  518]]\n",
            "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.18 - Evaluación del modelo en el conjunto de pruebas\n",
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7a05B8Gro6l",
        "outputId": "4344777e-d01e-4e28-fd9a-e45bb16755a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 154ms/step - loss: 0.6922 - acc: 0.5556\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6922480463981628, 0.5555555820465088]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.19 - Pseudocode RNN\n",
        "state_t = 0\n",
        "input_sequence = []\n",
        "for input_t in input_sequence:\n",
        "    output_t = f(input_t, state_t)\n",
        "    state_t = output_t"
      ],
      "metadata": {
        "id": "aF1t2Xmbry7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.22 - Preparando los datos de IMDB\n",
        "from keras.datasets import imdb\n",
        "#from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences\n",
        "import keras\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "batch_size = 32\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(\n",
        "num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = keras.utils.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = keras.utils.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN7a4LX1s5X1",
        "outputId": "0aef1102-d784-4d17-dc39-0fd59949ca9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "input_train shape: (25000, 500)\n",
            "input_test shape: (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.23 - Entrenar el modelo con capas Embedding y SimpleRNN\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP3pgrHAteAC",
        "outputId": "e3d31ee7-6820-4f4d-f439-58ea6caadc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 22s 137ms/step - loss: 0.6043 - acc: 0.6536 - val_loss: 0.4920 - val_acc: 0.7708\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 21s 132ms/step - loss: 0.3816 - acc: 0.8389 - val_loss: 0.3775 - val_acc: 0.8382\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 20s 129ms/step - loss: 0.2913 - acc: 0.8812 - val_loss: 0.4681 - val_acc: 0.7784\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 20s 129ms/step - loss: 0.2264 - acc: 0.9133 - val_loss: 0.3862 - val_acc: 0.8372\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 20s 129ms/step - loss: 0.1610 - acc: 0.9426 - val_loss: 0.3868 - val_acc: 0.8448\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 21s 132ms/step - loss: 0.1107 - acc: 0.9631 - val_loss: 0.5886 - val_acc: 0.7660\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 21s 131ms/step - loss: 0.0725 - acc: 0.9771 - val_loss: 0.7199 - val_acc: 0.8130\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0484 - acc: 0.9855 - val_loss: 0.6854 - val_acc: 0.7764\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 20s 131ms/step - loss: 0.0341 - acc: 0.9898 - val_loss: 0.5055 - val_acc: 0.8546\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 20s 130ms/step - loss: 0.0222 - acc: 0.9926 - val_loss: 0.7505 - val_acc: 0.7588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bGV6qY5SZK9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.24 - Trazado de resultados\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "otvmN8PDu6rt",
        "outputId": "f9c6d812-5aa9-4df5-b861-089c6c48854b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.27 - Uso de la capa LSTM en Keras\n",
        "from keras.layers import LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "        epochs=10,\n",
        "        batch_size=128,\n",
        "        validation_split=0.2)"
      ],
      "metadata": {
        "id": "8xOhx_nevfdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5675fc4b-c688-42f0-8830-9cbf5bb4840e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 48s 294ms/step - loss: 0.5096 - acc: 0.7623 - val_loss: 0.3789 - val_acc: 0.8350\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 46s 296ms/step - loss: 0.2914 - acc: 0.8850 - val_loss: 0.3878 - val_acc: 0.8586\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 51s 322ms/step - loss: 0.2344 - acc: 0.9125 - val_loss: 0.2820 - val_acc: 0.8840\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 47s 300ms/step - loss: 0.1998 - acc: 0.9244 - val_loss: 0.3522 - val_acc: 0.8568\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 46s 294ms/step - loss: 0.1802 - acc: 0.9348 - val_loss: 0.3085 - val_acc: 0.8796\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 46s 291ms/step - loss: 0.1583 - acc: 0.9430 - val_loss: 0.3118 - val_acc: 0.8826\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 45s 289ms/step - loss: 0.1494 - acc: 0.9464 - val_loss: 0.3730 - val_acc: 0.8608\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 45s 289ms/step - loss: 0.1311 - acc: 0.9540 - val_loss: 0.4309 - val_acc: 0.8634\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 46s 291ms/step - loss: 0.1229 - acc: 0.9577 - val_loss: 0.4594 - val_acc: 0.8592\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 46s 294ms/step - loss: 0.1113 - acc: 0.9615 - val_loss: 0.3429 - val_acc: 0.8658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.28 - Inspeccionar los datos del conjunto de datos de Jena\n",
        "import os\n",
        "data_dir = 'sample_data/jena_climate'\n",
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()\n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwDoB7zw3ccv",
        "outputId": "d97b960b-8476-47a0-9e08-ac250e30e83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date Time', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)', 'wd (deg)']\n",
            "499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.29 - Analizando los datos\n",
        "import numpy as np\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "dataset=[]\n",
        "for i, line in enumerate(lines):\n",
        "  #values = [float(x) for x in line.split(',')[1:]]\n",
        "  dataset.append(line)\n",
        "  #float_data[i, :] = values\n",
        "for value in dataset:\n",
        "    print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs1DnotG52a5",
        "outputId": "ca2751a2-19e7-4b15-9a8d-ff48407e21d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01.01.2009 00:10:00,996.52,-8.02,265.40,-8.90,93.30,3.33,3.11,0.22,1.94,3.12,1307.75,1.03,1.75,152.30\n",
            "01.01.2009 00:20:00,996.57,-8.41,265.01,-9.28,93.40,3.23,3.02,0.21,1.89,3.03,1309.80,0.72,1.50,136.10\n",
            "01.01.2009 00:30:00,996.53,-8.51,264.91,-9.31,93.90,3.21,3.01,0.20,1.88,3.02,1310.24,0.19,0.63,171.60\n",
            "01.01.2009 00:40:00,996.51,-8.31,265.12,-9.07,94.20,3.26,3.07,0.19,1.92,3.08,1309.19,0.34,0.50,198.00\n",
            "01.01.2009 00:50:00,996.51,-8.27,265.15,-9.04,94.10,3.27,3.08,0.19,1.92,3.09,1309.00,0.32,0.63,214.30\n",
            "01.01.2009 01:00:00,996.50,-8.05,265.38,-8.78,94.40,3.33,3.14,0.19,1.96,3.15,1307.86,0.21,0.63,192.70\n",
            "01.01.2009 01:10:00,996.50,-7.62,265.81,-8.30,94.80,3.44,3.26,0.18,2.04,3.27,1305.68,0.18,0.63,166.50\n",
            "01.01.2009 01:20:00,996.50,-7.62,265.81,-8.36,94.40,3.44,3.25,0.19,2.03,3.26,1305.69,0.19,0.50,118.60\n",
            "01.01.2009 01:30:00,996.50,-7.91,265.52,-8.73,93.80,3.36,3.15,0.21,1.97,3.16,1307.17,0.28,0.75,188.50\n",
            "01.01.2009 01:40:00,996.53,-8.43,264.99,-9.34,93.10,3.23,3.00,0.22,1.88,3.02,1309.85,0.59,0.88,185.00\n",
            "01.01.2009 01:50:00,996.62,-8.76,264.66,-9.66,93.10,3.14,2.93,0.22,1.83,2.94,1311.64,0.45,0.88,183.20\n",
            "01.01.2009 02:00:00,996.62,-8.88,264.54,-9.77,93.20,3.12,2.90,0.21,1.81,2.91,1312.25,0.25,0.63,190.30\n",
            "01.01.2009 02:10:00,996.63,-8.85,264.57,-9.70,93.50,3.12,2.92,0.20,1.82,2.93,1312.11,0.16,0.50,158.30\n",
            "01.01.2009 02:20:00,996.74,-8.83,264.58,-9.68,93.50,3.13,2.92,0.20,1.83,2.93,1312.15,0.36,0.63,184.80\n",
            "01.01.2009 02:30:00,996.81,-8.66,264.74,-9.46,93.90,3.17,2.98,0.19,1.86,2.99,1311.37,0.33,0.75,155.90\n",
            "01.01.2009 02:40:00,996.81,-8.66,264.74,-9.50,93.60,3.17,2.97,0.20,1.85,2.98,1311.38,0.07,0.50,272.40\n",
            "01.01.2009 02:50:00,996.86,-8.70,264.70,-9.55,93.50,3.16,2.95,0.21,1.85,2.96,1311.64,0.32,0.63,219.20\n",
            "01.01.2009 03:00:00,996.84,-8.81,264.59,-9.66,93.50,3.13,2.93,0.20,1.83,2.94,1312.18,0.18,0.63,167.20\n",
            "01.01.2009 03:10:00,996.87,-8.84,264.56,-9.69,93.50,3.13,2.92,0.20,1.83,2.93,1312.37,0.07,0.25,129.30\n",
            "01.01.2009 03:20:00,996.97,-8.94,264.45,-9.82,93.30,3.10,2.89,0.21,1.81,2.90,1313.01,0.10,0.63,115.30\n",
            "01.01.2009 03:30:00,997.08,-8.94,264.44,-9.80,93.40,3.10,2.90,0.20,1.81,2.90,1313.15,0.30,0.75,149.30\n",
            "01.01.2009 03:40:00,997.10,-8.86,264.52,-9.76,93.10,3.12,2.90,0.22,1.81,2.91,1312.78,0.29,0.75,149.70\n",
            "01.01.2009 03:50:00,997.06,-8.99,264.39,-9.99,92.40,3.09,2.85,0.23,1.78,2.86,1313.39,0.12,0.63,231.70\n",
            "01.01.2009 04:00:00,996.99,-9.05,264.34,-10.02,92.60,3.07,2.85,0.23,1.78,2.85,1313.61,0.10,0.38,240.00\n",
            "01.01.2009 04:10:00,997.05,-9.23,264.15,-10.25,92.20,3.03,2.79,0.24,1.74,2.80,1314.62,0.10,0.38,203.90\n",
            "01.01.2009 04:20:00,997.11,-9.49,263.89,-10.54,92.00,2.97,2.73,0.24,1.71,2.74,1316.02,0.34,0.75,159.70\n",
            "01.01.2009 04:30:00,997.19,-9.50,263.87,-10.51,92.30,2.97,2.74,0.23,1.71,2.75,1316.16,0.43,0.88,66.16\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.30 Plotting the temperature timeseries\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "data_dir = 'sample_data/jena_climate'\n",
        "fname = os.path.join(data_dir, '/content/sample_data/jena_climate_2009_2016.csv')\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "lines = data.split('\\n')\n",
        "\n",
        "# Temperature over the full temporal range of the dataset. Yearly trends are clearly visible\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "temp = float_data[:, 1] # <1> temperature (in degrees Celsius)\n",
        "plt.plot(range(len(temp)), temp)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "m_4ZU3IPXbf_",
        "outputId": "ba12bd14-0172-4cf7-d964-e66fa554e4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ad0a7d07daa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Temperature over the full temporal range of the dataset. Yearly trends are clearly visible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfloat_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# <1> temperature (in degrees Celsius)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'header' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.31 Plotting the first 10 days of the temperature timeseries\n",
        "plt.plot(range(1440), temp[:1440])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xcc08DEoY1rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.32 Normalizing the data\n",
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std"
      ],
      "metadata": {
        "id": "TkxT0fp1Y8LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.33 Generator yielding timeseries samples and their targets\n",
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "                shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(\n",
        "                    min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows),\n",
        "                            lookback // step,\n",
        "                            data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets"
      ],
      "metadata": {
        "id": "tihJgacNZAOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.34 Preparing the training, validation, and test generators\n",
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "\n",
        "train_gen = generator(float_data,\n",
        "                        lookback=lookback,\n",
        "                        delay=delay,\n",
        "                        min_index=0,\n",
        "                        max_index=200000,\n",
        "                        shuffle=True,\n",
        "                        step=step,\n",
        "                        batch_size=batch_size)\n",
        "val_gen = generator(float_data,\n",
        "                        lookback=lookback,\n",
        "                        delay=delay,\n",
        "                        min_index=200001,\n",
        "                        max_index=300000,\n",
        "                        step=step,\n",
        "                        batch_size=batch_size)\n",
        "test_gen = generator(float_data,\n",
        "                        lookback=lookback,\n",
        "                        delay=delay,\n",
        "                        min_index=300001,\n",
        "                        max_index=None,\n",
        "                        step=step,\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback)  // batch_size # How many steps to draw from\n",
        "            # val_gen in order to see the entire validation set\n",
        "test_steps = (len(float_data) - 300001 - lookback)  // batch_size"
      ],
      "metadata": {
        "id": "G-Z4I2rpZePP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.35 Computing the common-sense baseline MAE\n",
        "def evaluate_naive_method(): batch_maes = []\n",
        "for step in range(val_steps): samples, targets = next(val_gen)\n",
        "preds = samples[:, -1, 1]\n",
        "mae = np.mean(np.abs(preds - targets))\n",
        "batch_maes.append(mae)\n",
        "\n",
        "print(np.mean(batch_maes))\n",
        "evaluate_naive_method()"
      ],
      "metadata": {
        "id": "wKuJzQB-ZtHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.36 Converting the MAE back to a Celsius error\n",
        "celsius_mae = 0.29 * std[1]"
      ],
      "metadata": {
        "id": "x76PR1NcfUw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.37 Training and evaluating a densely connected model\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "\n",
        "history = model.fit_generator(train_gen,\n",
        "steps_per_epoch=500,\n",
        "epochs=20,\n",
        "validation_data=val_gen,\n",
        "validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "n-ieqZBYgVhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.38 Plotting results\n",
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RKHVf4LzgZac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.39 Training and evaluating a GRU-based model\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "steps_per_epoch=500,\n",
        "epochs=20,\n",
        "validation_data=val_gen,\n",
        "validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "vVx3GbnSgcm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.40 Training and evaluating a dropout-regularized GRU-based model\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "          dropout=0.2,\n",
        "          recurrent_dropout=0.2,\n",
        "          input_shape=(None, float_data.shape[-1])))\n",
        "\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "kLDsJFq3gfdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.41 Training and evaluating a dropout-regularized, stacked GRU model\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.GRU(32,\n",
        "          dropout=0.1,\n",
        "          recurrent_dropout=0.5,\n",
        "          return_sequences=True,\n",
        "          input_shape=(None, float_data.shape[-1])))\n",
        "\n",
        "model.add(layers.GRU(64, activation='relu',\n",
        "          dropout=0.1,\n",
        "          recurrent_dropout=0.5))\n",
        "\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "pjT0V5MeghUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.42 Training and evaluating an LSTM using reversed sequences\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = [x[::-1] for x in x_train]\n",
        "x_test = [x[::-1] for x in x_test]\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128))\n",
        "model.add(layers.LSTM(32))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "i9-IBucQgjab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.43 Training and evaluating a bidirectional LSTM\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 32))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "epochs=10,\n",
        "batch_size=128,\n",
        "validation_split=0.2)"
      ],
      "metadata": {
        "id": "XhXHnxeyglf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.44 Training a bidirectional GRU\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "model.add(layers.Bidirectional(\n",
        "layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "steps_per_epoch=500,\n",
        "epochs=40,\n",
        "validation_data=val_gen,\n",
        "validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "n46TK0wLgm8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.45 Preparing the IMDB data\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "id": "dDqi-sH7gpSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.46 Training and evaluating a simple 1D convnet on the IMDB data\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "epochs=10,\n",
        "batch_size=128,\n",
        "validation_split=0.2)"
      ],
      "metadata": {
        "id": "4kGld8ZRgqCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.47 Training and evaluating a simple 1D convnet on the Jena data\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation='relu',\n",
        "input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "steps_per_epoch=500,\n",
        "epochs=20,\n",
        "validation_data=val_gen,\n",
        "validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "WcnMYhLvgsyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.48 Preparing higher-resolution data generators for the Jena dataset\n",
        "step = 3\n",
        "lookback = 720\n",
        "delay = 144\n",
        "train_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=0,\n",
        "max_index=200000,\n",
        "shuffle=True,\n",
        "step=step)\n",
        "val_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=200001,\n",
        "max_index=300000,\n",
        "step=step)\n",
        "test_gen = generator(float_data,\n",
        "lookback=lookback,\n",
        "delay=delay,\n",
        "min_index=300001,\n",
        "max_index=None,\n",
        "step=step)\n",
        "val_steps = (300000 - 200001 - lookback) // 128\n",
        "test_steps = (len(float_data) - 300001 - lookback) // 128"
      ],
      "metadata": {
        "id": "asDPhJ1Fgur9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 6.49 Model combining a 1D convolutional base and a GRU layer\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation='relu',\n",
        "input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "steps_per_epoch=500,\n",
        "epochs=20,\n",
        "validation_data=val_gen,\n",
        "validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "09VB4Vv3gxU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "65a1095f-6945-475d-d8d4-705a1c6fa8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c22220ab036d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.add(layers.Conv1D(32, 5, activation='relu',\n\u001b[0;32m----> 8\u001b[0;31m input_shape=(None, float_data.shape[-1])))\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'float_data' is not defined"
          ]
        }
      ]
    }
  ]
}